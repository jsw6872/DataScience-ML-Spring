# 확률론
## 딥러닝에서 확률론의 필요성
- 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 둠
- 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도
  - 예측이 틀릴 위험을 최소화하도록 데이터를 학습하는 원리는 통계적 기계학습의 기본 원리
- 회귀분석에서 손실함수로 사용되는 L2-norm은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도
- 분류 문제에서 사용되는 cross-entropy는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도
- 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 $\phi$ 를 추출하게됨

## 몬테카를로 샘플링
- 기계학습의 많은 문제들은 확률분포를 명시적으로 모르는게 대부분
- 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링 방법을 사용(이산형, 연속형 상관없이 성립) 
  - 타겟으로 하는 f(x)에 데이터를 대입한 후에 산술평균을 계산, 단 독립적으로 샘플링해야함(대수의 법칙에 의해 수렴성을 보장)

[몬테카를로](https://m.blog.naver.com/rkdwnsdud555/220828040636)  
ex) 만약 파이를 모를 때
1. 넓이가 1인 정사각형 안에 반지름이 1인 4분원을 그린다  
2. 이때 4분원의 넓이는 x/4이다
3. 랜덤하게 다트를 m번던져서 원 안에 다트가 n개 들어온다면 원의 넓이는 대략적으로 x/4 = n/m
4. 즉, x = 4*n / m일 것이다

```python
import random
n=int(input("반복 횟수"))
count=0
for i in range(n):
    x=random.uniform(0, 1)
    y=random.uniform(0, 1)
    if (x ** 2 + y ** 2) <= 1:
        count = count + 1 
print("파이값은? ",4 * count / n)
```
